
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Retrieval-Augmented Generation &#8212; Programming for Lawyers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/RAG';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Appendix: Unsupervised Learning" href="unsupervised_learning.html" />
    <link rel="prev" title="Supervised Learning" href="supervised_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/uio_logo_small.svg" class="logo__image only-light" alt="Programming for Lawyers - Home"/>
    <script>document.write(`<img src="../_static/uio_logo_small.svg" class="logo__image only-dark" alt="Programming for Lawyers - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_basics.html">Programming basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_decisions.html">Decisions</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_lists.html">Lists</a></li>
<li class="toctree-l1"><a class="reference internal" href="03b_loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_functions.html">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="04b_dictionaries.html">Dictionaries</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_files_exceptions.html">Files and Exceptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tabular_data.html">Tabular Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="sorting_filtering.html">Sorting, Filtering Data and Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="information_extraction.html">Information Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="machine_learning.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervised_learning.html">Supervised Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Retrieval-Augmented Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="unsupervised_learning.html">Appendix: Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="JSON-APIs.html">Appendix: JSON and Web APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="standalone-programs.html">Appendix: Standalone Programs</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://jupyterhub.uio.no/hub/user-redirect/git-pull?repo=https%3A//github.com/UiO-CELL/programming-for-lawyers&urlpath=tree/programming-for-lawyers/docs/RAG.ipynb&branch=master" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="../_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/UiO-CELL/programming-for-lawyers/blob/master/docs/RAG.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/UiO-CELL/programming-for-lawyers" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/UiO-CELL/programming-for-lawyers/edit/master/docs/RAG.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/UiO-CELL/programming-for-lawyers/issues/new?title=Issue%20on%20page%20%2Fdocs/RAG.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/docs/RAG.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Retrieval-Augmented Generation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-software">Installing Software</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-language-model">The Language Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-storage-location">Model Storage Location</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface-login">HuggingFace Login</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-model">The Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-language-model">Using the Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vectorizer">The Vectorizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-location">Document location</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-documents">Loading the Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-documents">Splitting the Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-document-index">The Document Index</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-a-prompt">Making a Prompt</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-the-chatbot">Making the Chatbot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#asking-the-chatbot">Asking the Chatbot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="retrieval-augmented-generation">
<span id="rag"></span><h1>Retrieval-Augmented Generation<a class="headerlink" href="#retrieval-augmented-generation" title="Link to this heading">#</a></h1>
<p>Retrieval-Augmented Generation (RAG) is a method for including (parts of) matching documents as context for questions to a Large Language Model (LLM).
This can help reduce hallucinations and wrong answers.
A system for RAG has two major parts: a document database with a search index and a large language model.
<a class="reference internal" href="#rag-diagram"><span class="std std-numref">Fig. 5</span></a> shows the structure of our RAG program.</p>
<figure class="align-default" id="rag-diagram">
<img alt="The diagram shows some documents that are indexed by a search index. When the user enters a question, this is sent to the search index which retrieves matching documents parts. These document parts are then sent with the question as a prompt to language model. The language model answers the question based on the matching document parts." src="../_images/RAG.svg" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">The basic RAG architecture</span><a class="headerlink" href="#rag-diagram" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>When the user asks a question, the question is handled in two stages.
First, the question is used as a search query for the document database.
The search results are then sent together with the question to the LLM.
The LLM is prompted to answer the question based on the context in the search results.</p>
<p>We will use <a class="reference external" href="https://www.langchain.com/">LangChain</a>, an open-source library for making applications with LLMs.
This chapter was inspired by the article
<a class="reference external" href="https://medium.com/&#64;jiangan0808/retrieval-augmented-generation-rag-with-open-source-hugging-face-llms-using-langchain-bd618371be9d">Retrieval-Augmented Generation (RAG) with open-source Hugging Face LLMs using LangChain</a>.</p>
<section id="installing-software">
<h2>Installing Software<a class="headerlink" href="#installing-software" title="Link to this heading">#</a></h2>
<p>We’ll need to install some libraries first:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>sentence-transformers<span class="w"> </span>huggingface-hub<span class="w"> </span>faiss-cpu<span class="w"> </span>sentencepiece<span class="w"> </span>protobuf<span class="w"> </span>langchain<span class="w"> </span>langchain-community<span class="w"> </span>pypdf
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-language-model">
<h2>The Language Model<a class="headerlink" href="#the-language-model" title="Link to this heading">#</a></h2>
<p>We’ll use models from <a class="reference external" href="https://huggingface.co/">HuggingFace</a>, a website that has tools and models for machine learning.
We’ll use the open-weights LLM
<a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct">meta-llama/Llama-3.2-3B-Instruct</a>,
because it is small enough that we can use it with the smallest GPUs on Fox.
If you run on a GPU with more memory, you can get better results with a larger model, such as
<a class="reference external" href="https://huggingface.co/mistralai/Ministral-8B-Instruct-2410">mistralai/Ministral-8B-Instruct-2410</a>.</p>
<section id="model-storage-location">
<h3>Model Storage Location<a class="headerlink" href="#model-storage-location" title="Link to this heading">#</a></h3>
<p>We must download the model we want to use.
Because of the requirements mentioned above, we run our program on the <a class="reference external" href="https://www.uio.no/english/services/it/research/hpc/fox/">Fox</a> high-performance computer at UiO.
We must set the location where our program should store the models that we download from HuggingFace:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;HF_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;/fp/projects01/ec12/huggingface/cache/&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you run the program locally on your own computer, you might not need to set <code class="docutils literal notranslate"><span class="pre">HF_HOME</span></code>.</p>
</div>
</section>
<section id="huggingface-login">
<h3>HuggingFace Login<a class="headerlink" href="#huggingface-login" title="Link to this heading">#</a></h3>
<p>Even though the model Mistral-Nemo-Instruct-2407 is open source, we must log in to HuggingFace to download it.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>
<span class="n">login</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-model">
<h3>The Model<a class="headerlink" href="#the-model" title="Link to this heading">#</a></h3>
<p>Now, we are ready to download and use the model.
To use the model, we create a <em>pipeline</em>.
A pipeline can consist of several processing steps, but in this case, we only need one step.
We can use the method <code class="docutils literal notranslate"><span class="pre">HuggingFacePipeline.from_model_id()</span></code>, which automatically downloads the specified model from HuggingFace.</p>
<p>We check if we have a GPU available.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">device</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.llms</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="o">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="s1">&#39;meta-llama/Llama-3.2-3B-Instruct&#39;</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">&#39;text-generation&#39;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;max_new_tokens&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
        <span class="s1">&#39;do_sample&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
        <span class="s1">&#39;num_beams&#39;</span><span class="p">:</span> <span class="mi">4</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-pipeline-arguments admonition">
<p class="admonition-title">Pipeline Arguments</p>
<p>We give some arguments to the pipeline:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_id</span></code>: the name of the  model on HuggingFace</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task</span></code>:  the task you want to use the model for,  other alternatives are  translation and summarization</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device</span></code>: the GPU hardware device to use. If we don’t specify a device, no GPU will be used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pipeline_kwargs</span></code>: additional parameters that are passed to the model.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code>: maximum length of the generated text</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">do_sample</span></code>: by default, the most likely next word is chosen.  This makes the output deterministic. We can introduce some randomness by sampling among the  most likely words instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code>: the temperature controls the statistical <em>distribution</em> of the next word and is usually between 0 and 1. A low temperature increases the probability of common words. A high temperature increases the probability of outputting a rare word. Model makers often recommend a temperature setting, which we can use as a starting point.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_beams</span></code>: by default the model works with a single sequence of  tokens/words. With beam search, the program  builds multiple sequences at the same time, and then selects the best one in the end.</p></li>
</ul>
</li>
</ul>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you’re working on a computer with less memory, you might need to try a smaller model.
You can try for example <code class="docutils literal notranslate"><span class="pre">mistralai/Mistral-7B-Instruct-v0.3</span></code> or <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-1B-Instruct</span></code>.
The latter has only 1 billion parameters, and might be possible to use on a laptop, depending on how much memory it has.</p>
</div>
</section>
</section>
<section id="using-the-language-model">
<h2>Using the Language Model<a class="headerlink" href="#using-the-language-model" title="Link to this heading">#</a></h2>
<p>Now, the language model is ready to use.
Let’s try to use only the language model without RAG.
We can send it a query:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s1">&#39;what are the main problems with bitcoin?&#39;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This answer was generated based only on the information contained in the language model.
To improve the accuracy of the answer, we can provide the language model with additional context for our query.
To do that, we must load our document collection.</p>
</section>
<section id="the-vectorizer">
<h2>The Vectorizer<a class="headerlink" href="#the-vectorizer" title="Link to this heading">#</a></h2>
<p>Text must be <a class="reference internal" href="machine_learning.html#vectorizing"><span class="std std-ref">vectorized</span></a> before it can be processed.
Our HuggingFace pipeline will do that automatically for the large language model.
But we must make a vectorizer for the search index for our documents database.
We use a vectorizer called a word embedding model from HuggingFace.
Again, the HuggingFace library will automatically download the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">huggingface_embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;BAAI/bge-m3&#39;</span><span class="p">,</span>
    <span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda:0&#39;</span><span class="p">},</span>
    <span class="c1">#or: model_kwargs={&#39;device&#39;:&#39;cpu&#39;},</span>
    <span class="n">encode_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;normalize_embeddings&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-embeddings-arguments admonition">
<p class="admonition-title">Embeddings Arguments</p>
<p>These are the arguments to the embedding model:</p>
<ul class="simple">
<li><p>‘model_name’: the name of the model on HuggingFace</p></li>
<li><p>‘device’:  the hardware device to use, either a GPU or CPU</p></li>
<li><p>‘normalize_embeddings’:  embeddings can have different magnitudes. Normalizing the embeddings makes their magnitudes equal.</p></li>
</ul>
</div>
</section>
<section id="document-location">
<h2>Document location<a class="headerlink" href="#document-location" title="Link to this heading">#</a></h2>
<p>We have collected some papers licensed with a Creative Commons license.
We will try to load all the documents in the folder defined below.
If you prefer, you can change this to a different folder name.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">document_folder</span> <span class="o">=</span> <span class="s1">&#39;./documents/&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-the-documents">
<h2>Loading the Documents<a class="headerlink" href="#loading-the-documents" title="Link to this heading">#</a></h2>
<p>We use <code class="docutils literal notranslate"><span class="pre">DirectoryLoader</span></code> from LangChain to load all in files in <code class="docutils literal notranslate"><span class="pre">document_folder</span></code>.
<code class="docutils literal notranslate"><span class="pre">documents_folder</span></code> is defined above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">DirectoryLoader</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">DirectoryLoader</span><span class="p">(</span><span class="n">document_folder</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The document loader loads each file as a separate document.
We can check how long our documents are.
For example, we can use the function <code class="docutils literal notranslate"><span class="pre">max()</span></code> to find the length of the longest document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of documents:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Maximum document length: &#39;</span><span class="p">,</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>We can examine one of the documents:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="splitting-the-documents">
<h2>Splitting the Documents<a class="headerlink" href="#splitting-the-documents" title="Link to this heading">#</a></h2>
<p>Since we are only using PDFs with quite short pages, we can use them as they are.
Other, longer documents, for example the documents or webpages, we might need to split into chunks.
We can use a text splitter from LangChain to split documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.text_splitter</span><span class="w"> </span><span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">700</span><span class="p">,</span> <span class="c1">#  Could be more, for larger models like mistralai/Ministral-8B-Instruct-2410</span>
    <span class="n">chunk_overlap</span>  <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-text-splitter-arguments admonition">
<p class="admonition-title">Text  Splitter Arguments</p>
<p>These are the arguments to the text splitter:</p>
<ul class="simple">
<li><p>‘chunk_size’: the number of tokens in each chunk.  Not necessarily the same as the number of words.</p></li>
<li><p>‘chunk_overlap’: the number of tokens that are included in both chunks where the text is split.</p></li>
</ul>
</div>
<p>We can check if the maximum document length has changed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of documents:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Maximum document length: &#39;</span><span class="p">,</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-document-index">
<h2>The Document Index<a class="headerlink" href="#the-document-index" title="Link to this heading">#</a></h2>
<p>Next, we make a search index for our documents.
We will use this index for the retrieval part of ‘Retrieval-Augmented Generation’.
We use the open-source library <a class="reference external" href="https://github.com/facebookresearch/faiss">FAISS</a>
(Facebook AI Similarity Search) through LangChain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">FAISS</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">huggingface_embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>FAISS can find documents that match a search query:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">relevant_documents</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of documents found: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">relevant_documents</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can display the first document:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">relevant_documents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For our RAG application we need to access the search engine through an interface called a retriever:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;k&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-retriever-arguments admonition">
<p class="admonition-title">Retriever Arguments</p>
<p>These are the arguments to the retriever:</p>
<ul class="simple">
<li><p>‘k’: the number of documents to return (kNN search)</p></li>
</ul>
</div>
</section>
<section id="making-a-prompt">
<h2>Making a Prompt<a class="headerlink" href="#making-a-prompt" title="Link to this heading">#</a></h2>
<p>We can use a <em>prompt</em> to tell the language model how to answer.
The prompt should contain a few short, helpful instructions.
In addition, we provide placeholders for the context and the question.
LangChain replaces these with the actual context and question when we execute a query.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;You are an assistant for question-answering tasks.</span>
<span class="s1">Use the following pieces of retrieved context to answer the question.</span>
<span class="s1">Context: </span><span class="si">{context}</span>

<span class="s1">Question: </span><span class="si">{input}</span>

<span class="s1">Answer:</span>
<span class="s1">&#39;&#39;&#39;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
                        <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="making-the-chatbot">
<h2>Making the Chatbot<a class="headerlink" href="#making-the-chatbot" title="Link to this heading">#</a></h2>
<p>Now we can use the module <code class="docutils literal notranslate"><span class="pre">create_retrieval_chain</span></code> from LangChain to make an agent for answering questions, a chatbot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_retrieval_chain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains.combine_documents</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_stuff_documents_chain</span>

<span class="n">combine_documents_chain</span> <span class="o">=</span> <span class="n">create_stuff_documents_chain</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
<span class="n">rag_chain</span> <span class="o">=</span> <span class="n">create_retrieval_chain</span><span class="p">(</span><span class="n">retriever</span><span class="p">,</span> <span class="n">combine_documents_chain</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="asking-the-chatbot">
<h2>Asking the Chatbot<a class="headerlink" href="#asking-the-chatbot" title="Link to this heading">#</a></h2>
<p>Now, we can send our query to the chatbot.</p>
<div class="cell tag_scroll-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">&#39;input&#39;</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_scroll-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;answer&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Hopefully, this answer contains information from the context that
wasn’t in the previous answer, when we queried only the language model without RAG.</p>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Exercise: Use your own documents</p>
<p>Change the document location to your own documents folder.
You can also upload more documents that you want to try with RAG.
Change the query to a question that can be answered based on your documents.
Try to the run the query and evaluate the answer.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise: Saving the document index</p>
<p>The document index that we created with FAISS is only stored in memory.
To avoid having to reindex the documents every time we load the notebook, we can save the index.
Try to use the function <code class="docutils literal notranslate"><span class="pre">vectorstore.save_local()</span></code> to save the index.
Then, you can load the index from file using the function <code class="docutils literal notranslate"><span class="pre">FAISS.load_local()</span></code>.
See the documentation of the
<a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/faiss/#saving-and-loading">FAISS module in LangChain</a>
for further details.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise: Slurm Jobs</p>
<p>When you have made a program that works, it’s more efficient to run the program as a
<a class="reference external" href="https://www.uio.no/english/services/it/research/platforms/edu-research/help/fox/jobs/">batch job</a> than in JupyterLab.
This is because a JupyterLab session reserves a GPU all the time, also when you’re not running computations.
Therefore, you should save your finished program as a regular Python program that you can
<a class="reference external" href="https://training.pages.sigma2.no/tutorials/hpc-intro/episodes/13-scheduler.html">schedule</a> as a job.</p>
<p>You can save your code by clicking the “File”-menu in JupyterLab, click on “Save and Export Notebook As…” and then click “Executable Script”.
The result is the Python file <code class="docutils literal notranslate"><span class="pre">RAG.py</span></code> that is downloaded to your local computer.
You will also need to download the slurm script
<a class="reference download internal" download="" href="../_downloads/924747a7d7614a23fb5f153d3d74bf6b/LLM.slurm"><code class="xref download docutils literal notranslate"><span class="pre">LLM.slurm</span></code></a>.</p>
<p>Upload both the Python file <code class="docutils literal notranslate"><span class="pre">RAG.py</span></code> and the slurm script <code class="docutils literal notranslate"><span class="pre">LLM.slurm</span></code> to Fox.
Then, start the job with this command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>! sbatch LLM.slurm RAG.py
</pre></div>
</div>
<p>Slurm creates a log file for each job which is stored with a name like <code class="docutils literal notranslate"><span class="pre">slurm-1358473.out</span></code>.
By default, these log files are stored in the current working directory  where you run the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command.
If you want to store the log files somewhere else, you can add a line like below to your slurm script.
Remember to change the username.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#SBATCH --output=/fp/projects01/ec12/&lt;username&gt;/logs/slurm-%j.out
</pre></div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="supervised_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Supervised Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="unsupervised_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Appendix: Unsupervised Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-software">Installing Software</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-language-model">The Language Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-storage-location">Model Storage Location</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface-login">HuggingFace Login</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-model">The Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-language-model">Using the Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vectorizer">The Vectorizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-location">Document location</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-documents">Loading the Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-documents">Splitting the Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-document-index">The Document Index</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-a-prompt">Making a Prompt</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-the-chatbot">Making the Chatbot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#asking-the-chatbot">Asking the Chatbot</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Erik Winge
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>