{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b0f5c9-8070-44b8-b1ea-c0a337d373ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(RAG)=\n",
    "# Retrieval-Augmented Generation\n",
    "Retrieval-Augmented Generation (RAG) is a method for including (parts of) matching documents as context for questions to a Large Language Model (LLM).\n",
    "This can help reduce hallucinations and wrong answers.\n",
    "A system for RAG has two major parts: a document database with a search index and a large language model.\n",
    "{numref}`RAG-diagram` shows the structure of our RAG program.\n",
    "\n",
    "```{figure} ../images/RAG.svg\n",
    ":name: RAG-diagram\n",
    ":alt: The diagram shows some documents that are indexed by a search index. When the user enters a question, this is sent to the search index which retrieves matching documents parts. These document parts are then sent with the question as a prompt to language model. The language model answers the question based on the matching document parts.\n",
    "\n",
    "The basic RAG architecture\n",
    "```\n",
    "\n",
    "When the user asks a question, the question is handled in two stages.\n",
    "First, the question is used as a search query for the document database.\n",
    "The search results are then sent together with the question to the LLM.\n",
    "The LLM is prompted to answer the question based on the context in the search results.\n",
    "\n",
    "We will use [LangChain](https://www.langchain.com/), an open-source library for making applications with LLMs.\n",
    "This chapter was inspired by the article\n",
    "[Retrieval-Augmented Generation (RAG) with open-source Hugging Face LLMs using LangChain](\n",
    "https://medium.com/@jiangan0808/retrieval-augmented-generation-rag-with-open-source-hugging-face-llms-using-langchain-bd618371be9d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fa1bb-3bd0-4b32-9e1e-8fab3425c5a7",
   "metadata": {},
   "source": [
    "## Installing Software\n",
    "Weâ€™ll need to install some libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9908a60",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade sentence-transformers huggingface-hub faiss-cpu sentencepiece protobuf langchain langchain-community pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c01397b-edd3-44b4-8b92-d09da0c72b9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Language Model\n",
    "We'll use models from [HuggingFace](https://huggingface.co/), a website that has tools and models for machine learning.\n",
    "We'll use the open-weights LLM \n",
    "[meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct),\n",
    "because it is small enough that we can use it with the smallest GPUs on Fox.\n",
    "If you run on a GPU with more memory, you can get better results with a larger model, such as \n",
    "[mistralai/Ministral-8B-Instruct-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c239d4-1ebe-4bfa-85bc-8b946fa81b04",
   "metadata": {},
   "source": [
    "### Model Storage Location\n",
    "We must download the model we want to use.\n",
    "Because of the requirements mentioned above, we run our program on the [Fox](https://www.uio.no/english/services/it/research/hpc/fox/) high-performance computer at UiO.\n",
    "We must set the location where our program should store the models that we download from HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38b664-7075-4ed6-9a21-bb1465b7c095",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/fp/projects01/ec12/huggingface/cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c243e",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If you run the program locally on your own computer, you might not need to set `HF_HOME`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664d079-a478-4e47-804d-a4b9988723e4",
   "metadata": {},
   "source": [
    "### HuggingFace Login\n",
    "Even though the model Mistral-Nemo-Instruct-2407 is open source, we must log in to HuggingFace to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810905e-7214-43a8-818d-186c74aeddde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb595bd",
   "metadata": {},
   "source": [
    "### The Model\n",
    "Now, we are ready to download and use the model.\n",
    "To use the model, we create a *pipeline*.\n",
    "A pipeline can consist of several processing steps, but in this case, we only need one step.\n",
    "We can use the method `HuggingFacePipeline.from_model_id()`, which automatically downloads the specified model from HuggingFace.\n",
    "\n",
    "We check if we have a GPU available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016b6b9-db59-49ca-b639-c0bb2b53b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ccf84-09d3-4172-8236-5049285b3147",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#from langchain_huggingface.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b494217-4a06-46b1-9fc9-1156ad922846",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='meta-llama/Llama-3.2-3B-Instruct',\n",
    "    task='text-generation',\n",
    "    device=device,\n",
    "    pipeline_kwargs={\n",
    "        'max_new_tokens': 500,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.3,\n",
    "        'num_beams': 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42032173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{admonition} Pipeline Arguments\n",
    "We give some arguments to the pipeline:\n",
    "- `model_id`: the name of the  model on HuggingFace\n",
    "- `task`:  the task you want to use the model for,  other alternatives are  translation and summarization\n",
    "- `device`: the GPU hardware device to use. If we don't specify a device, no GPU will be used.\n",
    "- `pipeline_kwargs`: additional parameters that are passed to the model.\n",
    "    - `max_new_tokens`: maximum length of the generated text\n",
    "    - `do_sample`: by default, the most likely next word is chosen.  This makes the output deterministic. We can introduce some randomness by sampling among the  most likely words instead.\n",
    "    - `temperature`: the temperature controls the statistical *distribution* of the next word and is usually between 0 and 1. A low temperature increases the probability of common words. A high temperature increases the probability of outputting a rare word. Model makers often recommend a temperature setting, which we can use as a starting point.\n",
    "    - `num_beams`: by default the model works with a single sequence of  tokens/words. With beam search, the program  builds multiple sequences at the same time, and then selects the best one in the end.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0477a-8df7-4961-8890-fbf007e344c1",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "If you're working on a computer with less memory, you might need to try a smaller model.\n",
    "You can try for example `mistralai/Mistral-7B-Instruct-v0.3` or `meta-llama/Llama-3.2-1B-Instruct`.\n",
    "The latter has only 1 billion parameters, and might be possible to use on a laptop, depending on how much memory it has.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d0038",
   "metadata": {},
   "source": [
    "## Using the Language Model\n",
    "Now, the language model is ready to use.\n",
    "Let's try to use only the language model without RAG.\n",
    "We can send it a query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a2889-6746-4d18-8db0-38c3e9e2f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what are the main problems with bitcoin?'\n",
    "output = llm.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c68fab",
   "metadata": {},
   "source": [
    "This answer was generated based only on the information contained in the language model.\n",
    "To improve the accuracy of the answer, we can provide the language model with additional context for our query.\n",
    "To do that, we must load our document collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04780bbc-4a78-42fe-8c50-84beaf633a2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Language models sometimes produce markdown formatted text, like above.\n",
    "If you like, you can render this nicely with this function:\n",
    " \n",
    "    from IPython.display import Markdown\n",
    "    display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6b933",
   "metadata": {},
   "source": [
    "## The Vectorizer\n",
    "Text must be [vectorized](vectorizing) before it can be processed.\n",
    "Our HuggingFace pipeline will do that automatically for the large language model.\n",
    "But we must make a vectorizer for the search index for our documents database.\n",
    "We use a vectorizer called a word embedding model from HuggingFace.\n",
    "Again, the HuggingFace library will automatically download the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbff637-d938-445d-b769-a2122ded10f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-m3',\n",
    "    model_kwargs = {'device': 'cuda:0'},\n",
    "    #or: model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcde34e",
   "metadata": {},
   "source": [
    "```{admonition} Embeddings Arguments\n",
    "These are the arguments to the embedding model:\n",
    "- 'model_name': the name of the model on HuggingFace\n",
    "- 'device':  the hardware device to use, either a GPU or CPU\n",
    "- 'normalize_embeddings':  embeddings can have different magnitudes. Normalizing the embeddings makes their magnitudes equal.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb272ccc-e060-4cd0-b855-72c144503481",
   "metadata": {},
   "source": [
    "## Document location\n",
    "We have collected some papers licensed with a Creative Commons license.\n",
    "We will try to load all the documents in the folder defined below.\n",
    "If you prefer, you can change this to a different folder name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62dd1c-6122-45c1-90b3-16badbf11313",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_folder = './documents/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cccc3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Loading the Documents\n",
    "We use `DirectoryLoader` from LangChain to load all in files in `document_folder`.\n",
    "`documents_folder` is defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2716c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(document_folder)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c25011",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The document loader loads each file as a separate document.\n",
    "We can check how long our documents are. \n",
    "For example, we can use the function `max()` to find the length of the longest document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fb549-406c-44d0-9977-eaa29fb2645f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Number of documents:', len(documents))\n",
    "print('Maximum document length: ', max([len(doc.page_content) for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb87be6",
   "metadata": {},
   "source": [
    "We can examine one of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca436e9-389c-44b1-9b9c-22639b32fdef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77bd50",
   "metadata": {},
   "source": [
    "## Splitting the Documents\n",
    "Since we are only using PDFs with quite short pages, we can use them as they are.\n",
    "Other, longer documents, for example the documents or webpages, we might need to split into chunks. \n",
    "We can use a text splitter from LangChain to split documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a25aff-8375-4ced-9e97-99a004bdca47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700, #  Could be more, for larger models like mistralai/Ministral-8B-Instruct-2410\n",
    "    chunk_overlap  = 200,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75424e3-137e-425f-95a9-7fc9f9f1b984",
   "metadata": {},
   "source": [
    "```{admonition} Text  Splitter Arguments\n",
    "These are the arguments to the text splitter:\n",
    "- 'chunk_size': the number of tokens in each chunk.  Not necessarily the same as the number of words.\n",
    "- 'chunk_overlap': the number of tokens that are included in both chunks where the text is split.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8df41",
   "metadata": {},
   "source": [
    "We can check if the maximum document length has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b8845-4e0e-4b41-bdc6-c5b60aaaad86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Number of documents:', len(documents))\n",
    "print('Maximum document length: ', max([len(doc.page_content) for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9b6bb",
   "metadata": {},
   "source": [
    "## The Document Index\n",
    "Next, we make a search index for our documents.\n",
    "We will use this index for the retrieval part of 'Retrieval-Augmented Generation'.\n",
    "We use the open-source library [FAISS](https://github.com/facebookresearch/faiss)\n",
    "(Facebook AI Similarity Search) through LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb25e38-750d-490d-be4a-c15e6d789167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdf4cd",
   "metadata": {},
   "source": [
    "FAISS can find documents that match a search query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a7369-9690-4f13-84d2-2112d6da2e44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "print(f'Number of documents found: {len(relevant_documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a6f10-1912-428d-b793-023a75bfcdd4",
   "metadata": {},
   "source": [
    "We can display the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094fb4f-9337-43f6-86d6-1a5920a8bb9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957ab6b",
   "metadata": {},
   "source": [
    "For our RAG application we need to access the search engine through an interface called a retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6b860-1146-4256-8aad-5dd5cc245f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03857d",
   "metadata": {},
   "source": [
    "```{admonition} Retriever Arguments\n",
    "These are the arguments to the retriever:\n",
    "- 'k': the number of documents to return (kNN search)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e788a2",
   "metadata": {},
   "source": [
    "## Making a Prompt\n",
    "We can use a *prompt* to tell the language model how to answer.\n",
    "The prompt should contain a few short, helpful instructions.\n",
    "In addition, we provide placeholders for the context and the question.\n",
    "LangChain replaces these with the actual context and question when we execute a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f5c61-e016-4c26-9038-b063516835ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = '''You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "Context: {context}\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template,\n",
    "                        input_variables=['context', 'input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ed759",
   "metadata": {},
   "source": [
    "## Making the Chatbot\n",
    "Now we can use the module `create_retrieval_chain` from LangChain to make an agent for answering questions, a chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fec1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, combine_documents_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61b425",
   "metadata": {},
   "source": [
    "## Asking the Chatbot\n",
    "Now, we can send our query to the chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a29f4-d1da-4934-9229-1905613a38ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "result = rag_chain.invoke({'input': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22798b8-2bad-4a0b-b9a7-01f365890514",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c38ea6",
   "metadata": {},
   "source": [
    "Hopefully, this answer contains information from the context that\n",
    "wasn't in the previous answer, when we queried only the language model without RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30e9a6-ceec-4ed3-b5db-f8ab695e16b1",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab8202-baf2-4705-9c4e-6a0b7150a700",
   "metadata": {},
   "source": [
    "```{admonition} Exercise: Use your own documents\n",
    ":class: tip\n",
    "\n",
    "Change the document location to your own documents folder.\n",
    "You can also upload more documents that you want to try with RAG.\n",
    "Change the query to a question that can be answered based on your documents.\n",
    "Try to the run the query and evaluate the answer.\r\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71511b36-cb04-4d6b-83fb-6d1fe5494392",
   "metadata": {},
   "source": [
    "```{admonition} Exercise: Saving the document index\n",
    ":class: tip\n",
    "\n",
    "The document index that we created with FAISS is only stored in memory.\n",
    "To avoid having to reindex the documents every time we load the notebook, we can save the index.\n",
    "Try to use the function `vectorstore.save_local()` to save the index.\n",
    "Then, you can load the index from file using the function `FAISS.load_local()`.\n",
    "See the documentation of the \n",
    "[FAISS module in LangChain](https://python.langchain.com/docs/integrations/vectorstores/faiss/#saving-and-loading) \n",
    "for further details.\r\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb25cd7-1902-43b8-949b-92015fadcb19",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise: Slurm Jobs\n",
    ":class: tip\n",
    "\n",
    "When you have made a program that works, it's more efficient to run the program as a\n",
    "[batch job](https://www.uio.no/english/services/it/research/platforms/edu-research/help/fox/jobs/) than in JupyterLab.\n",
    "This is because a JupyterLab session reserves a GPU all the time, also when you're not running computations.\n",
    "Therefore, you should save your finished program as a regular Python program that you can\n",
    "[schedule](https://training.pages.sigma2.no/tutorials/hpc-intro/episodes/13-scheduler.html) as a job.\n",
    "\n",
    "You can save your code by clicking the \"File\"-menu in JupyterLab, click on \"Save and Export Notebook Asâ€¦\" and then click \"Executable Script\".\n",
    "The result is the Python file `RAG.py` that is downloaded to your local computer.\n",
    "You will also need to download the slurm script\n",
    "{download}`LLM.slurm<./LLM.slurm>`.\n",
    "\n",
    "Upload both the Python file `RAG.py` and the slurm script `LLM.slurm` to Fox.\n",
    "Then, start the job with this command:\n",
    "\n",
    "    ! sbatch LLM.slurm RAG.py\n",
    "\n",
    "Slurm creates a log file for each job which is stored with a name like `slurm-1358473.out`.\n",
    "By default, these log files are stored in the current working directory  where you run the `sbatch` command.\n",
    "If you want to store the log files somewhere else, you can add a line like below to your slurm script.\n",
    "Remember to change the username.\n",
    "\n",
    "    #SBATCH --output=/fp/projects01/ec12/<username>/logs/slurm-%j.out\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
